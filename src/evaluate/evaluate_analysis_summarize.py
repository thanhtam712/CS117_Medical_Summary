import os
import sys
import argparse
from pathlib import Path
from dotenv import load_dotenv
sys.path.append(str(Path(__file__).resolve().parent.parent))

from deepeval import evaluate
from deepeval.test_case import LLMTestCase
from deepeval.metrics import ContextualRecallMetric, ContextualPrecisionMetric, ContextualRelevancyMetric

load_dotenv()

def parse_args():
    parser = argparse.ArgumentParser(description="Evaluate Contextual Recall Performance")
    parser.add_argument(
        "--actual_output_path",
        type=str,
        required=True,
        help="The actual output generated by the model.",
    )
    parser.add_argument(
        "--expected_output_path",
        type=str,
        required=True,
        help="The expected output that the model should generate.",
    )
    parser.add_argument(
        "--retrieval_context_path",
        type=str,
        required=True,
        help="The context retrieved to assist in generating the expected output.",
    )
    parser.add_argument(
        "--output",
        type=str,
        required=True,
        help="The file to write the evaluation results to.",
    )
    return parser.parse_args()

def evaluate_contextual_recall(actual_output, expected_output, retrieval_context, output_file):
    metric_recall = ContextualRecallMetric(
        threshold=0.7,
        model="gpt-4.1-mini",
        include_reason=True,
    )
    
    metric_precision = ContextualPrecisionMetric(
        threshold=0.7,
        model="gpt-4.1-mini",
        include_reason=True,
    )
    
    metric_relevancy = ContextualRelevancyMetric(
        threshold=0.7,
        model="gpt-4.1-mini",
        include_reason=True,
    )
    
    test_case = LLMTestCase(
        input="What if these shoes don't fit?",
        actual_output=actual_output,
        expected_output=expected_output,
        retrieval_context=retrieval_context
    )

    metric_recall.measure(test_case)
    print(metric_recall.score, metric_recall.reason)
    
    metric_precision.measure(test_case)
    print(metric_precision.score, metric_precision.reason)
    
    metric_relevancy.measure(test_case)
    print(metric_relevancy.score, metric_relevancy.reason)
    
    with open(output_file, "a+", encoding="utf-8") as f:
        f.write(f"Contextual Recall: {metric_recall.score}\n")
        f.write(f"Contextual Precision: {metric_precision.score}\n")
        f.write(f"Contextual Relevancy: {metric_relevancy.score}\n")
    
    evaluate(test_cases=[test_case], metrics=[metric_recall, metric_precision, metric_relevancy])

if __name__ == "__main__":
    args = parse_args()
    actual_dir = Path(args.actual_output_path)
    expected_dir = Path(args.expected_output_path)
    retrieval_context_dir = Path(args.retrieval_context_path)
    output_dir = Path(args.output)
    
    for actual_file in actual_dir.iterdir():
        number = actual_file.stem.split("_")[-1]
        expected_file = expected_dir / f"test_result_groundtruth_{number}.txt"
        retrieval_context_file = retrieval_context_dir / f"test_result_{number}.txt"  

        data_actual = open(actual_file, encoding="utf-8").read().strip()
        data_expected = open(expected_file, encoding="utf-8").read().strip()
        data_retrieval_context = open(retrieval_context_file, encoding="utf-8").read().strip()
        
        output_file = output_dir / f"evaluation_results_{number}.txt"
        
        evaluate_contextual_recall(data_actual, data_expected, [data_retrieval_context], output_file)
    
